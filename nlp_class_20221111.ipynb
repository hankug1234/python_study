{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.5.1-cp310-cp310-win_amd64.whl (10.4 MB)\n",
      "     --------------------------------------- 10.4/10.4 MB 54.7 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "     ------------------------------------- 498.1/498.1 kB 32.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ygl\\anaconda3\\envs\\nbkim\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ygl\\anaconda3\\envs\\nbkim\\lib\\site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ygl\\anaconda3\\envs\\nbkim\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.5.1 pytz-2022.6\n"
     ]
    }
   ],
   "source": [
    "# ! pip install tensorflow\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " output_softmax (Dense)      (None, 20, 10)            20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape = (20, 1), name = \"input\")\n",
    "outputs = tf.keras.layers.Dense(units = 10, activation='softmax', name = \"output_softmax\")(inputs)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 20)]              0         \n",
      "                                                                 \n",
      " hidden1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 232\n",
      "Trainable params: 232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Hidden\n",
    "Input_size = 20\n",
    "inputs = tf.keras.layers.Input(shape = Input_size, name = \"input\")\n",
    "hidden1 = tf.keras.layers.Dense(units = 10, activation='relu',\n",
    "                                name = \"hidden1\")(inputs)\n",
    "# hidden1 = tf.keras.layers.Dense(units = 10, activation=tf.nn.relu, name = \"hidden1\")(inputs)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(units = 2, activation='softmax', name = 'output')(hidden1)\n",
    "# outputs = tf.keras.layers.Dense(units = 2, activation=tf.nn.softmax, name = 'output')(hidden1)\n",
    "\n",
    "\n",
    "model_2 = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 20)]              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 10)                210       \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 232\n",
      "Trainable params: 232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 20\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE, name = \"input\")\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2, name = 'dropout')(inputs)\n",
    "hidden = tf.keras.layers.Dense(units= 10, activation='relu', name = 'hidden')(dropout)\n",
    "outputs = tf.keras.layers.Dense(units= 2, activation='softmax', name = 'output')(hidden)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 20, 10)            40        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40\n",
      "Trainable params: 40\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1D conv layer\n",
    "INPUT_SIZE = (20, 1)\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "conv = tf.keras.layers.Conv1D(filters = 10,\n",
    "                             kernel_size=3, padding= \"same\",\n",
    "                             activation='relu')(inputs)\n",
    "\n",
    "model_4 = tf.keras.Model(inputs = inputs, outputs = conv)\n",
    "\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functionAPI\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 1)             0         \n",
      "                                                                 \n",
      " Conv (Conv1D)               (None, 20, 10)            40        \n",
      "                                                                 \n",
      " max_pool (MaxPooling1D)     (None, 6, 10)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 60)                0         \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 50)                3050      \n",
      "                                                                 \n",
      " outputs (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,600\n",
      "Trainable params: 3,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape = (20, 1), name = 'input')\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2, name = 'dropout')(inputs)\n",
    "conv = tf.keras.layers.Conv1D(filters = 10,\n",
    "                             kernel_size=3,\n",
    "                             padding = 'same',\n",
    "                             activation='relu',\n",
    "                             name = 'Conv')(dropout)\n",
    "max_pool = tf.keras.layers.MaxPool1D(pool_size=3, name = 'max_pool')(conv)\n",
    "flatten = tf.keras.layers.Flatten(name = 'flatten')(max_pool)\n",
    "hidden = tf.keras.layers.Dense(units = 50, activation='relu', name = 'hidden')(flatten)\n",
    "outputs = tf.keras.layers.Dense(units = 10, activation='softmax', \n",
    "                               name = 'outputs')(hidden)\n",
    "\n",
    "model_functionAPI = tf.keras.Model(inputs = inputs, outputs = outputs, \n",
    "                                  name = \"functionAPI\")\n",
    "model_functionAPI.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential_API\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense1 (Dense)              (None, 64)                2112      \n",
      "                                                                 \n",
      " Dense2 (Dense)              (None, 64)                4160      \n",
      "                                                                 \n",
      " Outputs (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,922\n",
      "Trainable params: 6,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Model 구축\n",
    "## Sequential\n",
    "\n",
    "model = tf.keras.Sequential(name = 'Sequential_API')\n",
    "model.add(tf.keras.layers.Input(shape = (32,), name = 'input'))\n",
    "model.add(tf.keras.layers.Dense(units = 64, activation = 'relu', name = 'Dense1'))\n",
    "model.add(tf.keras.layers.Dense(units = 64, activation='relu', name = 'Dense2'))\n",
    "model.add(tf.keras.layers.Dense(units = 10, activation='softmax', name = 'Outputs'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Functional_API\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 32)]              0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 64)                2112      \n",
      "                                                                 \n",
      " dense2 (Dense)              (None, 64)                4160      \n",
      "                                                                 \n",
      " outputs (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,922\n",
      "Trainable params: 6,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Functional API\n",
    "inputs = tf.keras.layers.Input(shape = (32,), name = 'input')\n",
    "Dense1 = tf.keras.layers.Dense(units = 64, activation='relu', name = 'dense1')(inputs)\n",
    "Dense2 = tf.keras.layers.Dense(units = 64, activation='relu', name = 'dense2')(Dense1)\n",
    "outputs = tf.keras.layers.Dense(units = 10, activation='softmax', name = 'outputs')(Dense2)\n",
    "\n",
    "model_function = tf.keras.Model(inputs = inputs, outputs = outputs, \n",
    "                                name = 'Functional_API')\n",
    "\n",
    "model_function.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Custom layers\n",
    "class Customlayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden1, hidden2, output_layer):\n",
    "        super(Customlayer, self).__init__()\n",
    "        self.hidden1 = hidden1\n",
    "        self.hidden2 = hidden2\n",
    "        self.output_layer = output_layer\n",
    "        \n",
    "    def build(self, inputs):\n",
    "        self.dense_layer1 = tf.keras.layers.Dense(units = self.hidden1, \n",
    "                                                 activation= \"relu\", name = 'Dense1')\n",
    "        self.dense_layer2 = tf.keras.layers.Dense(units = self.hidden2,\n",
    "                                                 activation='relu', name = 'Dense2')\n",
    "        self.output_layers = tf.keras.layers.Dense(units = self.output_layer,\n",
    "                                                  activation='softmax', name = 'output')\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_layer1(inputs)\n",
    "        x = self.dense_layer2(x)\n",
    "        \n",
    "        return self.output_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Custom_layer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " customlayer (Customlayer)   (None, 10)                6922      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,922\n",
      "Trainable params: 6,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential(name = 'Custom_layer')\n",
    "model.add(tf.keras.layers.Input(shape = (32,)))\n",
    "model.add(Customlayer(64, 64, 10))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subclassing\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, hidden1, hidden2, output_layers):\n",
    "        super(MyModel, self).__init__(name = 'MyModel')\n",
    "        self.dense1 = tf.keras.layers.Dense(units = hidden1, activation = 'relu',\n",
    "                                           name = 'dense1')\n",
    "        self.dense2 = tf.keras.layers.Dense(units = hidden2, activation = 'relu',\n",
    "                                           name = 'dense2')\n",
    "        self.output_layer = tf.keras.layers.Dense(units = output_layers,\n",
    "                                                 activation='softmax',\n",
    "                                                 name = 'output_layer')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_sub \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel_sub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nbkim\\lib\\site-packages\\keras\\engine\\training.py:3214\u001b[0m, in \u001b[0;36mModel.summary\u001b[1;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[0;32m   3184\u001b[0m \u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[0;32m   3185\u001b[0m \n\u001b[0;32m   3186\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[0;32m   3212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m-> 3214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3218\u001b[0m     )\n\u001b[0;32m   3219\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[0;32m   3220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3221\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3226\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[0;32m   3227\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model_sub = MyModel(64, 64, 10)\n",
    "\n",
    "# model_sub.summary()\n",
    "\n",
    "model_sub.complile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "model_sub.fit(train_input, train_target, epochs = 20,\n",
    "             batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.loadtxt('../dataset/pima-indians-diabetes.csv', delimiter = ',')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
      " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
      " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
      " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
      " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
      "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, -1]\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sequential\n",
    "\n",
    "# model = tf.keras.Sequential(name = 'Sequential')\n",
    "# model.add(tf.keras.layers.Input(shape = (8, ), name = 'input'))\n",
    "# model.add(tf.keras.layers.Dense(units = 12, activation= 'relu', name = 'dense1'))\n",
    "# model.add(tf.keras.layers.Dense(units = 8, activation= 'relu', name = 'dense2'))\n",
    "# model.add(tf.keras.layers.Dense(units = 1, activation= 'sigmoid', name = 'output'))\n",
    "\n",
    "## Functional API\n",
    "# inputs = tf.keras.layers.Input(shape = (8,), name = 'input')\n",
    "# dense1 = tf.keras.layers.Dense(units = 12, activation='relu', name = 'dense1')(inputs)\n",
    "# dense2 = tf.keras.layers.Dense(units = 8, activation='relu', name = 'dense2')(dense1)\n",
    "# outputs = tf.keras.layers.Dense(units =1 , activation='sigmoid', name = 'outputs')(dense2)\n",
    "# model = tf.keras.Model(inputs = inputs, outputs = outputs, name = 'Functional_API')\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "class Mymodel(tf.keras.Model):\n",
    "    def __init__(self, hidden1, hidden2, outputs):\n",
    "        super(Mymodel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units = hidden1, activation=tf.nn.relu, name = 'dense1')\n",
    "        self.dense2 = tf.keras.layers.Dense(units = hidden2, activation=tf.nn.relu, name = 'dense2')\n",
    "        self.outputs = tf.keras.layers.Dense(units = outputs, activation=tf.nn.sigmoid, name = 'output')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return self.outputs(x)\n",
    "\n",
    "model = Mymodel(12, 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = './model_save/\\\n",
    "{epoch:2d}_{val_loss:.4f}.hdf5', monitor = 'val_loss',\n",
    "                                               verbose = 1,\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                                 patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/9 [==>...........................] - ETA: 2s - loss: 10.1987 - acc: 0.3714\n",
      "Epoch 1: val_loss improved from 10.54315 to 5.33362, saving model to ./model_save\\ 1_5.3336.hdf5\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 8.4997 - acc: 0.3485 - val_loss: 5.3336 - val_acc: 0.4351\n",
      "Epoch 2/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7.2199 - acc: 0.3714\n",
      "Epoch 2: val_loss improved from 5.33362 to 2.76299, saving model to ./model_save\\ 2_2.7630.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 4.3146 - acc: 0.4186 - val_loss: 2.7630 - val_acc: 0.5455\n",
      "Epoch 3/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4.1684 - acc: 0.4857\n",
      "Epoch 3: val_loss improved from 2.76299 to 1.99900, saving model to ./model_save\\ 3_1.9990.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 2.7443 - acc: 0.5717 - val_loss: 1.9990 - val_acc: 0.6494\n",
      "Epoch 4/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1.8923 - acc: 0.6429\n",
      "Epoch 4: val_loss improved from 1.99900 to 1.61940, saving model to ./model_save\\ 4_1.6194.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 2.1046 - acc: 0.6205 - val_loss: 1.6194 - val_acc: 0.6883\n",
      "Epoch 5/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1.5347 - acc: 0.6429\n",
      "Epoch 5: val_loss improved from 1.61940 to 1.27709, saving model to ./model_save\\ 5_1.2771.hdf5\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 1.6340 - acc: 0.6368 - val_loss: 1.2771 - val_acc: 0.6688\n",
      "Epoch 6/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1.5763 - acc: 0.6286\n",
      "Epoch 6: val_loss improved from 1.27709 to 1.14295, saving model to ./model_save\\ 6_1.1430.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 1.3417 - acc: 0.6319 - val_loss: 1.1430 - val_acc: 0.6558\n",
      "Epoch 7/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1.3859 - acc: 0.6714\n",
      "Epoch 7: val_loss improved from 1.14295 to 1.09816, saving model to ./model_save\\ 7_1.0982.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 1.2589 - acc: 0.6189 - val_loss: 1.0982 - val_acc: 0.6364\n",
      "Epoch 8/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1.5517 - acc: 0.5571\n",
      "Epoch 8: val_loss improved from 1.09816 to 1.05715, saving model to ./model_save\\ 8_1.0571.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 1.2067 - acc: 0.6189 - val_loss: 1.0571 - val_acc: 0.6494\n",
      "Epoch 9/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1.7482 - acc: 0.5571\n",
      "Epoch 9: val_loss improved from 1.05715 to 1.01080, saving model to ./model_save\\ 9_1.0108.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 1.1596 - acc: 0.6319 - val_loss: 1.0108 - val_acc: 0.6688\n",
      "Epoch 10/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.9913 - acc: 0.6714\n",
      "Epoch 10: val_loss improved from 1.01080 to 0.96387, saving model to ./model_save\\10_0.9639.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 1.1156 - acc: 0.6368 - val_loss: 0.9639 - val_acc: 0.6818\n",
      "Epoch 11/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1.8104 - acc: 0.5571\n",
      "Epoch 11: val_loss improved from 0.96387 to 0.91888, saving model to ./model_save\\11_0.9189.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 1.0786 - acc: 0.6450 - val_loss: 0.9189 - val_acc: 0.6753\n",
      "Epoch 12/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.7983 - acc: 0.7000\n",
      "Epoch 12: val_loss improved from 0.91888 to 0.88626, saving model to ./model_save\\12_0.8863.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 1.0330 - acc: 0.6466 - val_loss: 0.8863 - val_acc: 0.6753\n",
      "Epoch 13/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.9302 - acc: 0.6571\n",
      "Epoch 13: val_loss improved from 0.88626 to 0.85385, saving model to ./model_save\\13_0.8539.hdf5\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.9973 - acc: 0.6515 - val_loss: 0.8539 - val_acc: 0.6818\n",
      "Epoch 14/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1.0063 - acc: 0.6714\n",
      "Epoch 14: val_loss improved from 0.85385 to 0.83308, saving model to ./model_save\\14_0.8331.hdf5\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.9641 - acc: 0.6531 - val_loss: 0.8331 - val_acc: 0.6753\n",
      "Epoch 15/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.9627 - acc: 0.6000\n",
      "Epoch 15: val_loss improved from 0.83308 to 0.79322, saving model to ./model_save\\15_0.7932.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9302 - acc: 0.6612 - val_loss: 0.7932 - val_acc: 0.6883\n",
      "Epoch 16/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1.3030 - acc: 0.6000\n",
      "Epoch 16: val_loss improved from 0.79322 to 0.76313, saving model to ./model_save\\16_0.7631.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.8954 - acc: 0.6694 - val_loss: 0.7631 - val_acc: 0.6818\n",
      "Epoch 17/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6875 - acc: 0.7143\n",
      "Epoch 17: val_loss improved from 0.76313 to 0.73755, saving model to ./model_save\\17_0.7376.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.8641 - acc: 0.6775 - val_loss: 0.7376 - val_acc: 0.6883\n",
      "Epoch 18/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.8020 - acc: 0.6571\n",
      "Epoch 18: val_loss improved from 0.73755 to 0.72471, saving model to ./model_save\\18_0.7247.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.8409 - acc: 0.6775 - val_loss: 0.7247 - val_acc: 0.6818\n",
      "Epoch 19/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1.0082 - acc: 0.5429\n",
      "Epoch 19: val_loss improved from 0.72471 to 0.71043, saving model to ./model_save\\19_0.7104.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.8197 - acc: 0.6792 - val_loss: 0.7104 - val_acc: 0.6818\n",
      "Epoch 20/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.8655 - acc: 0.6714\n",
      "Epoch 20: val_loss improved from 0.71043 to 0.69862, saving model to ./model_save\\20_0.6986.hdf5\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.7982 - acc: 0.6775 - val_loss: 0.6986 - val_acc: 0.6818\n",
      "Epoch 21/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.9303 - acc: 0.6000\n",
      "Epoch 21: val_loss improved from 0.69862 to 0.68508, saving model to ./model_save\\21_0.6851.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.7804 - acc: 0.6759 - val_loss: 0.6851 - val_acc: 0.6883\n",
      "Epoch 22/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.8640 - acc: 0.6286\n",
      "Epoch 22: val_loss improved from 0.68508 to 0.67500, saving model to ./model_save\\22_0.6750.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.7668 - acc: 0.6726 - val_loss: 0.6750 - val_acc: 0.6883\n",
      "Epoch 23/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.7723 - acc: 0.7000\n",
      "Epoch 23: val_loss did not improve from 0.67500\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.7505 - acc: 0.6726 - val_loss: 0.6782 - val_acc: 0.7013\n",
      "Epoch 24/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.9511 - acc: 0.5714\n",
      "Epoch 24: val_loss improved from 0.67500 to 0.66154, saving model to ./model_save\\24_0.6615.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.7375 - acc: 0.6759 - val_loss: 0.6615 - val_acc: 0.6818\n",
      "Epoch 25/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6856 - acc: 0.6714\n",
      "Epoch 25: val_loss improved from 0.66154 to 0.65509, saving model to ./model_save\\25_0.6551.hdf5\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.7208 - acc: 0.6808 - val_loss: 0.6551 - val_acc: 0.6948\n",
      "Epoch 26/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6812 - acc: 0.6429\n",
      "Epoch 26: val_loss improved from 0.65509 to 0.64668, saving model to ./model_save\\26_0.6467.hdf5\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.7068 - acc: 0.6743 - val_loss: 0.6467 - val_acc: 0.6948\n",
      "Epoch 27/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5871 - acc: 0.7286\n",
      "Epoch 27: val_loss improved from 0.64668 to 0.64183, saving model to ./model_save\\27_0.6418.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6960 - acc: 0.6759 - val_loss: 0.6418 - val_acc: 0.6883\n",
      "Epoch 28/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6423 - acc: 0.7000\n",
      "Epoch 28: val_loss improved from 0.64183 to 0.63400, saving model to ./model_save\\28_0.6340.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6867 - acc: 0.6840 - val_loss: 0.6340 - val_acc: 0.6948\n",
      "Epoch 29/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.7046 - acc: 0.7000\n",
      "Epoch 29: val_loss improved from 0.63400 to 0.63160, saving model to ./model_save\\29_0.6316.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6777 - acc: 0.6824 - val_loss: 0.6316 - val_acc: 0.6948\n",
      "Epoch 30/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.7469 - acc: 0.6714\n",
      "Epoch 30: val_loss improved from 0.63160 to 0.62793, saving model to ./model_save\\30_0.6279.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6696 - acc: 0.6889 - val_loss: 0.6279 - val_acc: 0.6818\n",
      "Epoch 31/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5568 - acc: 0.7571\n",
      "Epoch 31: val_loss improved from 0.62793 to 0.62624, saving model to ./model_save\\31_0.6262.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6624 - acc: 0.6873 - val_loss: 0.6262 - val_acc: 0.6883\n",
      "Epoch 32/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5434 - acc: 0.7857\n",
      "Epoch 32: val_loss improved from 0.62624 to 0.62304, saving model to ./model_save\\32_0.6230.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6556 - acc: 0.6873 - val_loss: 0.6230 - val_acc: 0.6688\n",
      "Epoch 33/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.7614 - acc: 0.6571\n",
      "Epoch 33: val_loss improved from 0.62304 to 0.62171, saving model to ./model_save\\33_0.6217.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6515 - acc: 0.6873 - val_loss: 0.6217 - val_acc: 0.6688\n",
      "Epoch 34/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6455 - acc: 0.6714\n",
      "Epoch 34: val_loss improved from 0.62171 to 0.62055, saving model to ./model_save\\34_0.6206.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6448 - acc: 0.6938 - val_loss: 0.6206 - val_acc: 0.6688\n",
      "Epoch 35/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5523 - acc: 0.7286\n",
      "Epoch 35: val_loss did not improve from 0.62055\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.6421 - acc: 0.6922 - val_loss: 0.6218 - val_acc: 0.6494\n",
      "Epoch 36/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6589 - acc: 0.6286\n",
      "Epoch 36: val_loss did not improve from 0.62055\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.6365 - acc: 0.6922 - val_loss: 0.6214 - val_acc: 0.6494\n",
      "Epoch 37/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5542 - acc: 0.7429\n",
      "Epoch 37: val_loss improved from 0.62055 to 0.61847, saving model to ./model_save\\37_0.6185.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6315 - acc: 0.6987 - val_loss: 0.6185 - val_acc: 0.6494\n",
      "Epoch 38/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5135 - acc: 0.7000\n",
      "Epoch 38: val_loss improved from 0.61847 to 0.61590, saving model to ./model_save\\38_0.6159.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6291 - acc: 0.7003 - val_loss: 0.6159 - val_acc: 0.6494\n",
      "Epoch 39/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6418 - acc: 0.6714\n",
      "Epoch 39: val_loss did not improve from 0.61590\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.6259 - acc: 0.7003 - val_loss: 0.6163 - val_acc: 0.6494\n",
      "Epoch 40/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5815 - acc: 0.6857\n",
      "Epoch 40: val_loss improved from 0.61590 to 0.61460, saving model to ./model_save\\40_0.6146.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6207 - acc: 0.7003 - val_loss: 0.6146 - val_acc: 0.6558\n",
      "Epoch 41/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5660 - acc: 0.7000\n",
      "Epoch 41: val_loss improved from 0.61460 to 0.61158, saving model to ./model_save\\41_0.6116.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6174 - acc: 0.7085 - val_loss: 0.6116 - val_acc: 0.6364\n",
      "Epoch 42/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.7155 - acc: 0.6571\n",
      "Epoch 42: val_loss improved from 0.61158 to 0.60991, saving model to ./model_save\\42_0.6099.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6158 - acc: 0.7085 - val_loss: 0.6099 - val_acc: 0.6429\n",
      "Epoch 43/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6541 - acc: 0.6857\n",
      "Epoch 43: val_loss did not improve from 0.60991\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.6127 - acc: 0.7134 - val_loss: 0.6115 - val_acc: 0.6429\n",
      "Epoch 44/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6947 - acc: 0.7000\n",
      "Epoch 44: val_loss did not improve from 0.60991\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.6111 - acc: 0.7150 - val_loss: 0.6119 - val_acc: 0.6364\n",
      "Epoch 45/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6417 - acc: 0.7286\n",
      "Epoch 45: val_loss improved from 0.60991 to 0.60833, saving model to ./model_save\\45_0.6083.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6117 - acc: 0.7134 - val_loss: 0.6083 - val_acc: 0.6558\n",
      "Epoch 46/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6010 - acc: 0.7571\n",
      "Epoch 46: val_loss did not improve from 0.60833\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.6088 - acc: 0.7199 - val_loss: 0.6101 - val_acc: 0.6364\n",
      "Epoch 47/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5800 - acc: 0.7286\n",
      "Epoch 47: val_loss improved from 0.60833 to 0.60749, saving model to ./model_save\\47_0.6075.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6047 - acc: 0.7117 - val_loss: 0.6075 - val_acc: 0.6494\n",
      "Epoch 48/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5508 - acc: 0.7571\n",
      "Epoch 48: val_loss improved from 0.60749 to 0.60696, saving model to ./model_save\\48_0.6070.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6034 - acc: 0.7134 - val_loss: 0.6070 - val_acc: 0.6364\n",
      "Epoch 49/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6399 - acc: 0.7286\n",
      "Epoch 49: val_loss improved from 0.60696 to 0.60486, saving model to ./model_save\\49_0.6049.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.6003 - acc: 0.7117 - val_loss: 0.6049 - val_acc: 0.6299\n",
      "Epoch 50/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.7066 - acc: 0.6286\n",
      "Epoch 50: val_loss improved from 0.60486 to 0.60378, saving model to ./model_save\\50_0.6038.hdf5\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.6013 - acc: 0.7215 - val_loss: 0.6038 - val_acc: 0.6364\n",
      "Epoch 51/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5991 - acc: 0.7714\n",
      "Epoch 51: val_loss did not improve from 0.60378\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.6004 - acc: 0.7182 - val_loss: 0.6044 - val_acc: 0.6688\n",
      "Epoch 52/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6150 - acc: 0.7143\n",
      "Epoch 52: val_loss did not improve from 0.60378\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5942 - acc: 0.7215 - val_loss: 0.6059 - val_acc: 0.6364\n",
      "Epoch 53/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5798 - acc: 0.7429\n",
      "Epoch 53: val_loss improved from 0.60378 to 0.60147, saving model to ./model_save\\53_0.6015.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.5988 - acc: 0.7199 - val_loss: 0.6015 - val_acc: 0.6494\n",
      "Epoch 54/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6017 - acc: 0.7429\n",
      "Epoch 54: val_loss improved from 0.60147 to 0.60115, saving model to ./model_save\\54_0.6011.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.5928 - acc: 0.7182 - val_loss: 0.6011 - val_acc: 0.6364\n",
      "Epoch 55/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6246 - acc: 0.6571\n",
      "Epoch 55: val_loss did not improve from 0.60115\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5921 - acc: 0.7150 - val_loss: 0.6029 - val_acc: 0.6494\n",
      "Epoch 56/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5497 - acc: 0.7143\n",
      "Epoch 56: val_loss improved from 0.60115 to 0.59856, saving model to ./model_save\\56_0.5986.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 5ms/step - loss: 0.5913 - acc: 0.7150 - val_loss: 0.5986 - val_acc: 0.6429\n",
      "Epoch 57/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5478 - acc: 0.7286\n",
      "Epoch 57: val_loss did not improve from 0.59856\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5909 - acc: 0.7182 - val_loss: 0.6011 - val_acc: 0.6364\n",
      "Epoch 58/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6042 - acc: 0.7429\n",
      "Epoch 58: val_loss did not improve from 0.59856\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5863 - acc: 0.7182 - val_loss: 0.6003 - val_acc: 0.6623\n",
      "Epoch 59/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5532 - acc: 0.6714\n",
      "Epoch 59: val_loss did not improve from 0.59856\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5861 - acc: 0.7166 - val_loss: 0.5990 - val_acc: 0.6429\n",
      "Epoch 60/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5651 - acc: 0.7429\n",
      "Epoch 60: val_loss improved from 0.59856 to 0.59737, saving model to ./model_save\\60_0.5974.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.5835 - acc: 0.7199 - val_loss: 0.5974 - val_acc: 0.6429\n",
      "Epoch 61/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5682 - acc: 0.7714\n",
      "Epoch 61: val_loss improved from 0.59737 to 0.59650, saving model to ./model_save\\61_0.5965.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.5846 - acc: 0.7068 - val_loss: 0.5965 - val_acc: 0.6558\n",
      "Epoch 62/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6338 - acc: 0.6857\n",
      "Epoch 62: val_loss did not improve from 0.59650\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5841 - acc: 0.7166 - val_loss: 0.5975 - val_acc: 0.6429\n",
      "Epoch 63/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5785 - acc: 0.7429\n",
      "Epoch 63: val_loss did not improve from 0.59650\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5794 - acc: 0.7182 - val_loss: 0.5969 - val_acc: 0.6623\n",
      "Epoch 64/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5155 - acc: 0.7714\n",
      "Epoch 64: val_loss improved from 0.59650 to 0.59616, saving model to ./model_save\\64_0.5962.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.5795 - acc: 0.7280 - val_loss: 0.5962 - val_acc: 0.6299\n",
      "Epoch 65/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6553 - acc: 0.6571\n",
      "Epoch 65: val_loss improved from 0.59616 to 0.59488, saving model to ./model_save\\65_0.5949.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.5799 - acc: 0.7134 - val_loss: 0.5949 - val_acc: 0.6494\n",
      "Epoch 66/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5132 - acc: 0.7571\n",
      "Epoch 66: val_loss did not improve from 0.59488\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5767 - acc: 0.7264 - val_loss: 0.5971 - val_acc: 0.6623\n",
      "Epoch 67/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5350 - acc: 0.7857\n",
      "Epoch 67: val_loss did not improve from 0.59488\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5751 - acc: 0.7248 - val_loss: 0.5950 - val_acc: 0.6299\n",
      "Epoch 68/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5177 - acc: 0.7714\n",
      "Epoch 68: val_loss did not improve from 0.59488\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5756 - acc: 0.7166 - val_loss: 0.5950 - val_acc: 0.6299\n",
      "Epoch 69/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5460 - acc: 0.7857\n",
      "Epoch 69: val_loss did not improve from 0.59488\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5709 - acc: 0.7280 - val_loss: 0.5962 - val_acc: 0.6558\n",
      "Epoch 70/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5695 - acc: 0.6857\n",
      "Epoch 70: val_loss did not improve from 0.59488\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5704 - acc: 0.7231 - val_loss: 0.5960 - val_acc: 0.6429\n",
      "Epoch 71/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5325 - acc: 0.7714\n",
      "Epoch 71: val_loss did not improve from 0.59488\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5702 - acc: 0.7231 - val_loss: 0.5964 - val_acc: 0.6429\n",
      "Epoch 72/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5806 - acc: 0.7429\n",
      "Epoch 72: val_loss did not improve from 0.59488\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5722 - acc: 0.7264 - val_loss: 0.5968 - val_acc: 0.6623\n",
      "Epoch 73/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.4951 - acc: 0.8000\n",
      "Epoch 73: val_loss did not improve from 0.59488\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5699 - acc: 0.7101 - val_loss: 0.5952 - val_acc: 0.6429\n",
      "Epoch 74/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5557 - acc: 0.6429\n",
      "Epoch 74: val_loss improved from 0.59488 to 0.59456, saving model to ./model_save\\74_0.5946.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.5699 - acc: 0.7345 - val_loss: 0.5946 - val_acc: 0.6558\n",
      "Epoch 75/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6048 - acc: 0.7571\n",
      "Epoch 75: val_loss improved from 0.59456 to 0.59394, saving model to ./model_save\\75_0.5939.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.5674 - acc: 0.7345 - val_loss: 0.5939 - val_acc: 0.6429\n",
      "Epoch 76/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5246 - acc: 0.7143\n",
      "Epoch 76: val_loss did not improve from 0.59394\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5650 - acc: 0.7280 - val_loss: 0.5962 - val_acc: 0.6558\n",
      "Epoch 77/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5314 - acc: 0.7429\n",
      "Epoch 77: val_loss did not improve from 0.59394\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5657 - acc: 0.7248 - val_loss: 0.5949 - val_acc: 0.6429\n",
      "Epoch 78/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5341 - acc: 0.7000\n",
      "Epoch 78: val_loss did not improve from 0.59394\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5668 - acc: 0.7280 - val_loss: 0.5941 - val_acc: 0.6429\n",
      "Epoch 79/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5238 - acc: 0.8143\n",
      "Epoch 79: val_loss did not improve from 0.59394\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5646 - acc: 0.7313 - val_loss: 0.5952 - val_acc: 0.6494\n",
      "Epoch 80/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5713 - acc: 0.7000\n",
      "Epoch 80: val_loss improved from 0.59394 to 0.59370, saving model to ./model_save\\80_0.5937.hdf5\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.5652 - acc: 0.7280 - val_loss: 0.5937 - val_acc: 0.6494\n",
      "Epoch 81/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6309 - acc: 0.6571\n",
      "Epoch 81: val_loss did not improve from 0.59370\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5635 - acc: 0.7329 - val_loss: 0.5947 - val_acc: 0.6429\n",
      "Epoch 82/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6140 - acc: 0.6857\n",
      "Epoch 82: val_loss did not improve from 0.59370\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5608 - acc: 0.7345 - val_loss: 0.5948 - val_acc: 0.6688\n",
      "Epoch 83/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.6528 - acc: 0.6714\n",
      "Epoch 83: val_loss did not improve from 0.59370\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5626 - acc: 0.7280 - val_loss: 0.5949 - val_acc: 0.6623\n",
      "Epoch 84/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5162 - acc: 0.7857\n",
      "Epoch 84: val_loss did not improve from 0.59370\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5601 - acc: 0.7313 - val_loss: 0.5950 - val_acc: 0.6494\n",
      "Epoch 85/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.4966 - acc: 0.7714\n",
      "Epoch 85: val_loss did not improve from 0.59370\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5597 - acc: 0.7345 - val_loss: 0.5958 - val_acc: 0.6494\n",
      "Epoch 86/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.4693 - acc: 0.7857\n",
      "Epoch 86: val_loss did not improve from 0.59370\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5585 - acc: 0.7296 - val_loss: 0.5959 - val_acc: 0.6558\n",
      "Epoch 87/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5576 - acc: 0.7429\n",
      "Epoch 87: val_loss did not improve from 0.59370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5576 - acc: 0.7264 - val_loss: 0.5942 - val_acc: 0.6429\n",
      "Epoch 88/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.4840 - acc: 0.7714\n",
      "Epoch 88: val_loss did not improve from 0.59370\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5570 - acc: 0.7313 - val_loss: 0.5951 - val_acc: 0.6688\n",
      "Epoch 89/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.4971 - acc: 0.7857\n",
      "Epoch 89: val_loss did not improve from 0.59370\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5570 - acc: 0.7264 - val_loss: 0.5955 - val_acc: 0.6558\n",
      "Epoch 90/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5615 - acc: 0.7000\n",
      "Epoch 90: val_loss did not improve from 0.59370\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.5548 - acc: 0.7280 - val_loss: 0.5943 - val_acc: 0.6429\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['acc'])\n",
    "history = model.fit(X, Y, epochs = 200, batch_size = 70, validation_split=0.2,\n",
    "         callbacks=[checkpoint, early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mymodel_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense1 (Dense)              multiple                  108       \n",
      "                                                                 \n",
      " dense2 (Dense)              multiple                  104       \n",
      "                                                                 \n",
      " output (Dense)              multiple                  9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(history.history['val_acc'])\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.6.2-cp310-cp310-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 51.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ygl\\anaconda3\\envs\\nbkim\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.6-cp310-cp310-win_amd64.whl (163 kB)\n",
      "     ---------------------------------------- 163.6/163.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\ygl\\anaconda3\\envs\\nbkim\\lib\\site-packages (from matplotlib) (1.23.4)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "     ------------------------------------- 965.4/965.4 kB 59.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ygl\\anaconda3\\envs\\nbkim\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.3.0-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 76.7 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-win_amd64.whl (55 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ygl\\anaconda3\\envs\\nbkim\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ygl\\anaconda3\\envs\\nbkim\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.0.6 cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.2 pillow-9.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['val_loss'], '-r', label = 'Validation loss')\n",
    "plt.plot(history.history['val_acc'], '-b', label = 'Validation acc')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
